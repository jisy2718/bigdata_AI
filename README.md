 



안녕하세요. Spark 실습 따라한 코드를 올리는 곳입니다.



## [spark폴더](./spark)

+ s[spark_dataframe_exercise_01](./spark/spark_dataframe_exercise_01)
  + Pandas DF와 비교
  + csv load, Spark DF 생성
  + printSchema(), null 개수 확인
  + Select(), filter()

+ [spark_dataframe_exercise_02](./spark/spark_dataframe_exercise_02)
  + orderBy(), groupBy(), agg(), upper(), col(), alias()

+ [spark_dataframe_exercise_03](./spark/spark_dataframe_exercise_03)
  + 신규 컬럼 추가, 컬럼과 레코드 삭제, null 처리, 사용자 정의 함수

+ [spark_iris_basic](./spark/spark_iris_basic)
  + Spark ML process, Pipeline